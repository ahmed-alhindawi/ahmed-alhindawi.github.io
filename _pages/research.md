---
layout: page
title: Research
permalink: /research/
image: 
---

<!--Iâ€™m an Assistant Professor at the [Dept. of Computer Science](https://comp.nus.edu.sg){:target="_blank"}, [National University of Singapore](https://www.nus.edu.sg){:target="_blank"} (NUS), where I direct the [Collaborative, Learning, and Adaptive Robots (CLeAR)](https://clear-nus.github.io/trust){:target="_blank"} Lab. -->

My goal is to reduce the burden of neuro-cognitive diseases for patients in intensive care.

> Specifically, how can we diagnose and assist those patients as best as possible

My general approach is to understand the disease process as much as possible, then ask question on how to best combine state-of-the-art in engineering and biology to help those with that disease.

I'm particularly interested in disorders of the brains and muscles of patients in intensive care in an assistive way. During my graduate studies, I built an eye tracking platform that can reliably diagnose patients in intensive care with delirium (confusion) and more recently, I've been exploring how robots can assist patients with muscle weakness.. 

<!-- <img align="right" style="padding: 20px 15px 0px 10px;" width="400" src="/images/nurse_wound_small.jpg">
This image of a nurse helping to clean a patient's wound summarizes the challenges I seek to address. How can we enable robots to assist in such a task? And equally important, can we assure the  human patient so that they would cooperate with the robot and not refuse assistance (if the robot is indeed capable of helping)? The answers to these questions reveal important clues about what is missing in today's robots. I believe two of the most crucial pieces are:
- **Touch Intelligence**: To help appropriately, the robot needs to make sense of the tactile sensations it receives to avoid causing hurt or harm to the patient. Can the robot learn to apply the right amount of pressure to clean the wound, but not cause pain? Compared to vision, the sense of touch remains underdeveloped in robots. 
- **Trust in the robot**: Imagine if you cut your hand; would you allow a robot to clean your wound? Why or why not? I would argue your *trust* in the robot plays an important role in your decision, and to assist appropriately, the robot needs to understand when and how humans trust.  

Most of my work in the past few years has centered on these two topics. Together with my team at [CLeAR](https://clear-nus.github.io){:target="_blank"}, we have made important strides in giving robots a [sense of touch](https://clear-nus.github.io/tactile){:target="_blank"} and a [sense of trust](https://clear-nus.github.io/trust){:target="_blank"}. To develop solutions, we build machine-learning models for perception and inference (whether tactile or trust) --- I'm particularly fond of [deep generative models](https://clear-nus.github.io/generativemodels){:target="_blank"} and latent state-space models that enable robots to have imagination and simulate possible futures. We also run human-robot-interaction (HRI) experiments to gain inspiration and validate that our methods actually work with people. We have been fortunate to receive recognition for our work on these topics in the form of best paper nominations and press mentions.   -->


### Research Awards


### Invited Talks


### Selected Press and Interviews




